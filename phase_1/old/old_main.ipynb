{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75a13f4c",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a704be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tickers for Korean stocks\n",
    "tickers = {\n",
    "    \"ì‚¼ì„±ì „ì\": \"005930\", \"SK\": \"034730\", \"í•œí™”\": \"000880\",\n",
    "    \"ë‘ì‚°\": \"000150\", \"ê¸°ì•„\": \"000270\", \"í˜„ëŒ€ì°¨\": \"005380\",\n",
    "    \"LG\": \"003550\", \"NAVER\": \"035420\", \"ì¹´ì¹´ì˜¤\": \"035720\", \"ë¡¯ë°ì§€ì£¼\": \"004990\"\n",
    "}\n",
    "\n",
    "# Date range for the stock data\n",
    "start_date = \"20200101\"\n",
    "end_date = \"20250101\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7376c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target ticker for analysis\n",
    "TARGET_TICKER = \"ì‚¼ì„±ì „ì\"\n",
    "ticker_code = tickers[TARGET_TICKER]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea70de00",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "01a7cf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading stock data for ticker: ì‚¼ì„±ì „ì (005930) ---\n",
      "--- Loading OHLCV data from KRX for ticker: ì‚¼ì„±ì „ì (005930) ---\n",
      "--- Adding technical indicators using 'ta' library ---\n",
      "\n",
      "--- Data shape after adding indicators: (1228, 92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devwo\\miniconda3\\Lib\\site-packages\\ta\\trend.py:988: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '54945.28' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self._psar.iloc[i] = self._psar.iloc[i - 1] + (\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>kospi_close</th>\n",
       "      <th>target_label</th>\n",
       "      <th>news</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>volume_adi</th>\n",
       "      <th>volume_obv</th>\n",
       "      <th>volume_cmf</th>\n",
       "      <th>volume_fi</th>\n",
       "      <th>...</th>\n",
       "      <th>momentum_ppo</th>\n",
       "      <th>momentum_ppo_signal</th>\n",
       "      <th>momentum_ppo_hist</th>\n",
       "      <th>momentum_pvo</th>\n",
       "      <th>momentum_pvo_signal</th>\n",
       "      <th>momentum_pvo_hist</th>\n",
       "      <th>momentum_kama</th>\n",
       "      <th>others_dr</th>\n",
       "      <th>others_dlr</th>\n",
       "      <th>others_cr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>55200</td>\n",
       "      <td>2175.17</td>\n",
       "      <td>1</td>\n",
       "      <td>[ì‚¼ì„±ì „ì, CES2020ì„œ ê²Œì´ë° ëª¨ë‹ˆí„° â€˜ì˜¤ë””ì„¸ì´â€™ ì‹ ëª¨ë¸ ì²« ê³µê°œ - Sams...</td>\n",
       "      <td>0.257930</td>\n",
       "      <td>-7.795937e+06</td>\n",
       "      <td>12993228</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55200.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>55500</td>\n",
       "      <td>2176.46</td>\n",
       "      <td>1</td>\n",
       "      <td>[ì‚¼ì„±ì „ì, CES2020ì„œ ê²Œì´ë° ëª¨ë‹ˆí„° â€˜ì˜¤ë””ì„¸ì´â€™ ì‹ ëª¨ë¸ ì²« ê³µê°œ - Sams...</td>\n",
       "      <td>0.433907</td>\n",
       "      <td>-1.233189e+07</td>\n",
       "      <td>28415483</td>\n",
       "      <td>-0.433985</td>\n",
       "      <td>4.626676e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043337</td>\n",
       "      <td>0.008667</td>\n",
       "      <td>0.034670</td>\n",
       "      <td>1.470935</td>\n",
       "      <td>0.294187</td>\n",
       "      <td>1.176748</td>\n",
       "      <td>55201.248699</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.542007</td>\n",
       "      <td>0.543478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>55500</td>\n",
       "      <td>2155.07</td>\n",
       "      <td>2</td>\n",
       "      <td>[ì‚¼ì„±ì „ìê°€ ì—´ì–´ê°ˆ ë¯¸ë˜ëŠ”? CES 2020 í‚¤ë…¸íŠ¸ ìš”ì•½ì •ë¦¬ - Samsung Ne...</td>\n",
       "      <td>0.244561</td>\n",
       "      <td>-4.108733e+06</td>\n",
       "      <td>38694434</td>\n",
       "      <td>-0.106184</td>\n",
       "      <td>3.965723e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076768</td>\n",
       "      <td>0.022287</td>\n",
       "      <td>0.054480</td>\n",
       "      <td>-0.516397</td>\n",
       "      <td>0.132070</td>\n",
       "      <td>-0.648467</td>\n",
       "      <td>55202.492201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.543478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>55800</td>\n",
       "      <td>2175.54</td>\n",
       "      <td>2</td>\n",
       "      <td>[ì‚¼ì„±ì „ìê°€ ì—´ì–´ê°ˆ ë¯¸ë˜ëŠ”? CES 2020 í‚¤ë…¸íŠ¸ ìš”ì•½ì •ë¦¬ - Samsung Ne...</td>\n",
       "      <td>0.314477</td>\n",
       "      <td>-9.113622e+06</td>\n",
       "      <td>48704212</td>\n",
       "      <td>-0.187122</td>\n",
       "      <td>3.828181e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145310</td>\n",
       "      <td>0.046892</td>\n",
       "      <td>0.098418</td>\n",
       "      <td>-2.290921</td>\n",
       "      <td>-0.352528</td>\n",
       "      <td>-1.938393</td>\n",
       "      <td>55468.051223</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.539085</td>\n",
       "      <td>1.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>56800</td>\n",
       "      <td>2151.31</td>\n",
       "      <td>2</td>\n",
       "      <td>[â€œë¯¸ë˜ì—ì„œ ì˜¨ ê²Œì´ë° ëª¨ë‹ˆí„°â€ ì‚¼ì„± â€˜ì˜¤ë””ì„¸ì´â€™ ë””ìì¸ ìŠ¤í† ë¦¬ - Samsung ...</td>\n",
       "      <td>0.496105</td>\n",
       "      <td>-4.413388e+06</td>\n",
       "      <td>72205383</td>\n",
       "      <td>-0.061123</td>\n",
       "      <td>6.638608e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.341003</td>\n",
       "      <td>0.105714</td>\n",
       "      <td>0.235289</td>\n",
       "      <td>4.516718</td>\n",
       "      <td>0.621321</td>\n",
       "      <td>3.895397</td>\n",
       "      <td>56060.028457</td>\n",
       "      <td>1.792115</td>\n",
       "      <td>1.776246</td>\n",
       "      <td>2.898551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>2024-12-18</td>\n",
       "      <td>54900</td>\n",
       "      <td>2484.43</td>\n",
       "      <td>0</td>\n",
       "      <td>[ì‚¼ì„±ì „ì, CES 2025ì„œ â€˜AI í™ˆâ€™ íƒ‘ì¬í•œ ìŠ¤í¬ë¦° ê°€ì „ ëŒ€ê±° ê³µê°œ - Sam...</td>\n",
       "      <td>0.109803</td>\n",
       "      <td>-1.710382e+09</td>\n",
       "      <td>372085605</td>\n",
       "      <td>0.028801</td>\n",
       "      <td>2.044246e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.922780</td>\n",
       "      <td>-1.375726</td>\n",
       "      <td>0.452945</td>\n",
       "      <td>-11.465406</td>\n",
       "      <td>-7.371424</td>\n",
       "      <td>-4.093982</td>\n",
       "      <td>69203.659073</td>\n",
       "      <td>1.291513</td>\n",
       "      <td>1.283244</td>\n",
       "      <td>-0.543478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>2024-12-19</td>\n",
       "      <td>53100</td>\n",
       "      <td>2435.93</td>\n",
       "      <td>2</td>\n",
       "      <td>[ç¾, ì‚¼ì„±ì „ì ë³´ì¡°ê¸ˆ ìµœì¢… ê²°ì •â€¦26% ì¤„ì–´ë“  47.5ì–µ ë‹¬ëŸ¬ ì§€ê¸‰ - ì¤‘ì•™ì¼ë³´,...</td>\n",
       "      <td>0.171850</td>\n",
       "      <td>-1.732863e+09</td>\n",
       "      <td>349603680</td>\n",
       "      <td>-0.045420</td>\n",
       "      <td>-4.028856e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.105438</td>\n",
       "      <td>-1.321668</td>\n",
       "      <td>0.216230</td>\n",
       "      <td>-9.969824</td>\n",
       "      <td>-7.891104</td>\n",
       "      <td>-2.078720</td>\n",
       "      <td>69136.630315</td>\n",
       "      <td>-3.278689</td>\n",
       "      <td>-3.333642</td>\n",
       "      <td>-3.804348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>2024-12-20</td>\n",
       "      <td>53000</td>\n",
       "      <td>2404.15</td>\n",
       "      <td>2</td>\n",
       "      <td>[ç¾, ì‚¼ì„±ì „ì ë³´ì¡°ê¸ˆ ìµœì¢… ê²°ì •â€¦26% ì¤„ì–´ë“  47.5ì–µ ë‹¬ëŸ¬ ì§€ê¸‰ - ì¤‘ì•™ì¼ë³´,...</td>\n",
       "      <td>0.383987</td>\n",
       "      <td>-1.712301e+09</td>\n",
       "      <td>324928906</td>\n",
       "      <td>0.026063</td>\n",
       "      <td>-3.805802e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.251883</td>\n",
       "      <td>-1.307711</td>\n",
       "      <td>0.055828</td>\n",
       "      <td>-7.869492</td>\n",
       "      <td>-7.886782</td>\n",
       "      <td>0.017290</td>\n",
       "      <td>69069.464320</td>\n",
       "      <td>-0.188324</td>\n",
       "      <td>-0.188501</td>\n",
       "      <td>-3.985507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>53500</td>\n",
       "      <td>2442.01</td>\n",
       "      <td>1</td>\n",
       "      <td>[ì‚¼ì„±ì „ì, ë¯¸êµ­ ë°˜ë„ì²´ ë³´ì¡°ê¸ˆ 7ì¡°ì› ë°›ëŠ”ë‹¤ - ë¸”ë¡œí„°, SKí•˜ì´ë‹‰ìŠ¤, ë‚´ë…„ì—” ì‚¼...</td>\n",
       "      <td>0.352443</td>\n",
       "      <td>-1.718161e+09</td>\n",
       "      <td>338601556</td>\n",
       "      <td>-0.070158</td>\n",
       "      <td>-2.285498e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.279904</td>\n",
       "      <td>-1.302150</td>\n",
       "      <td>0.022245</td>\n",
       "      <td>-10.259350</td>\n",
       "      <td>-8.361296</td>\n",
       "      <td>-1.898055</td>\n",
       "      <td>69004.659057</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.938974</td>\n",
       "      <td>-3.079710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>2024-12-24</td>\n",
       "      <td>54400</td>\n",
       "      <td>2440.52</td>\n",
       "      <td>0</td>\n",
       "      <td>[ì‚¼ì„±ì „ì, CES 2025ì„œ â€˜ê°€ì •ìš© íˆíŠ¸íŒí”„ EHSâ€™ ç¾ ì‹œì¥ì— ì²« ì„ ë´¬ - S...</td>\n",
       "      <td>0.205407</td>\n",
       "      <td>-1.709112e+09</td>\n",
       "      <td>350236233</td>\n",
       "      <td>-0.058435</td>\n",
       "      <td>-4.631111e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.155257</td>\n",
       "      <td>-1.272771</td>\n",
       "      <td>0.117514</td>\n",
       "      <td>-13.008088</td>\n",
       "      <td>-9.290654</td>\n",
       "      <td>-3.717434</td>\n",
       "      <td>68943.869634</td>\n",
       "      <td>1.682243</td>\n",
       "      <td>1.668250</td>\n",
       "      <td>-1.449275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1228 rows Ã— 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  close  kospi_close  target_label  \\\n",
       "0    2020-01-02  55200      2175.17             1   \n",
       "1    2020-01-03  55500      2176.46             1   \n",
       "2    2020-01-06  55500      2155.07             2   \n",
       "3    2020-01-07  55800      2175.54             2   \n",
       "4    2020-01-08  56800      2151.31             2   \n",
       "...         ...    ...          ...           ...   \n",
       "1223 2024-12-18  54900      2484.43             0   \n",
       "1224 2024-12-19  53100      2435.93             2   \n",
       "1225 2024-12-20  53000      2404.15             2   \n",
       "1226 2024-12-23  53500      2442.01             1   \n",
       "1227 2024-12-24  54400      2440.52             0   \n",
       "\n",
       "                                                   news  sentiment  \\\n",
       "0     [ì‚¼ì„±ì „ì, CES2020ì„œ ê²Œì´ë° ëª¨ë‹ˆí„° â€˜ì˜¤ë””ì„¸ì´â€™ ì‹ ëª¨ë¸ ì²« ê³µê°œ - Sams...   0.257930   \n",
       "1     [ì‚¼ì„±ì „ì, CES2020ì„œ ê²Œì´ë° ëª¨ë‹ˆí„° â€˜ì˜¤ë””ì„¸ì´â€™ ì‹ ëª¨ë¸ ì²« ê³µê°œ - Sams...   0.433907   \n",
       "2     [ì‚¼ì„±ì „ìê°€ ì—´ì–´ê°ˆ ë¯¸ë˜ëŠ”? CES 2020 í‚¤ë…¸íŠ¸ ìš”ì•½ì •ë¦¬ - Samsung Ne...   0.244561   \n",
       "3     [ì‚¼ì„±ì „ìê°€ ì—´ì–´ê°ˆ ë¯¸ë˜ëŠ”? CES 2020 í‚¤ë…¸íŠ¸ ìš”ì•½ì •ë¦¬ - Samsung Ne...   0.314477   \n",
       "4     [â€œë¯¸ë˜ì—ì„œ ì˜¨ ê²Œì´ë° ëª¨ë‹ˆí„°â€ ì‚¼ì„± â€˜ì˜¤ë””ì„¸ì´â€™ ë””ìì¸ ìŠ¤í† ë¦¬ - Samsung ...   0.496105   \n",
       "...                                                 ...        ...   \n",
       "1223  [ì‚¼ì„±ì „ì, CES 2025ì„œ â€˜AI í™ˆâ€™ íƒ‘ì¬í•œ ìŠ¤í¬ë¦° ê°€ì „ ëŒ€ê±° ê³µê°œ - Sam...   0.109803   \n",
       "1224  [ç¾, ì‚¼ì„±ì „ì ë³´ì¡°ê¸ˆ ìµœì¢… ê²°ì •â€¦26% ì¤„ì–´ë“  47.5ì–µ ë‹¬ëŸ¬ ì§€ê¸‰ - ì¤‘ì•™ì¼ë³´,...   0.171850   \n",
       "1225  [ç¾, ì‚¼ì„±ì „ì ë³´ì¡°ê¸ˆ ìµœì¢… ê²°ì •â€¦26% ì¤„ì–´ë“  47.5ì–µ ë‹¬ëŸ¬ ì§€ê¸‰ - ì¤‘ì•™ì¼ë³´,...   0.383987   \n",
       "1226  [ì‚¼ì„±ì „ì, ë¯¸êµ­ ë°˜ë„ì²´ ë³´ì¡°ê¸ˆ 7ì¡°ì› ë°›ëŠ”ë‹¤ - ë¸”ë¡œí„°, SKí•˜ì´ë‹‰ìŠ¤, ë‚´ë…„ì—” ì‚¼...   0.352443   \n",
       "1227  [ì‚¼ì„±ì „ì, CES 2025ì„œ â€˜ê°€ì •ìš© íˆíŠ¸íŒí”„ EHSâ€™ ç¾ ì‹œì¥ì— ì²« ì„ ë´¬ - S...   0.205407   \n",
       "\n",
       "        volume_adi  volume_obv  volume_cmf     volume_fi  ...  momentum_ppo  \\\n",
       "0    -7.795937e+06    12993228   -0.600000  0.000000e+00  ...      0.000000   \n",
       "1    -1.233189e+07    28415483   -0.433985  4.626676e+09  ...      0.043337   \n",
       "2    -4.108733e+06    38694434   -0.106184  3.965723e+09  ...      0.076768   \n",
       "3    -9.113622e+06    48704212   -0.187122  3.828181e+09  ...      0.145310   \n",
       "4    -4.413388e+06    72205383   -0.061123  6.638608e+09  ...      0.341003   \n",
       "...            ...         ...         ...           ...  ...           ...   \n",
       "1223 -1.710382e+09   372085605    0.028801  2.044246e+09  ...     -0.922780   \n",
       "1224 -1.732863e+09   349603680   -0.045420 -4.028856e+09  ...     -1.105438   \n",
       "1225 -1.712301e+09   324928906    0.026063 -3.805802e+09  ...     -1.251883   \n",
       "1226 -1.718161e+09   338601556   -0.070158 -2.285498e+09  ...     -1.279904   \n",
       "1227 -1.709112e+09   350236233   -0.058435 -4.631111e+08  ...     -1.155257   \n",
       "\n",
       "      momentum_ppo_signal  momentum_ppo_hist  momentum_pvo  \\\n",
       "0                0.000000           0.000000      0.000000   \n",
       "1                0.008667           0.034670      1.470935   \n",
       "2                0.022287           0.054480     -0.516397   \n",
       "3                0.046892           0.098418     -2.290921   \n",
       "4                0.105714           0.235289      4.516718   \n",
       "...                   ...                ...           ...   \n",
       "1223            -1.375726           0.452945    -11.465406   \n",
       "1224            -1.321668           0.216230     -9.969824   \n",
       "1225            -1.307711           0.055828     -7.869492   \n",
       "1226            -1.302150           0.022245    -10.259350   \n",
       "1227            -1.272771           0.117514    -13.008088   \n",
       "\n",
       "      momentum_pvo_signal  momentum_pvo_hist  momentum_kama  others_dr  \\\n",
       "0                0.000000           0.000000   55200.000000   0.000000   \n",
       "1                0.294187           1.176748   55201.248699   0.543478   \n",
       "2                0.132070          -0.648467   55202.492201   0.000000   \n",
       "3               -0.352528          -1.938393   55468.051223   0.540541   \n",
       "4                0.621321           3.895397   56060.028457   1.792115   \n",
       "...                   ...                ...            ...        ...   \n",
       "1223            -7.371424          -4.093982   69203.659073   1.291513   \n",
       "1224            -7.891104          -2.078720   69136.630315  -3.278689   \n",
       "1225            -7.886782           0.017290   69069.464320  -0.188324   \n",
       "1226            -8.361296          -1.898055   69004.659057   0.943396   \n",
       "1227            -9.290654          -3.717434   68943.869634   1.682243   \n",
       "\n",
       "      others_dlr  others_cr  \n",
       "0       0.000000   0.000000  \n",
       "1       0.542007   0.543478  \n",
       "2       0.000000   0.543478  \n",
       "3       0.539085   1.086957  \n",
       "4       1.776246   2.898551  \n",
       "...          ...        ...  \n",
       "1223    1.283244  -0.543478  \n",
       "1224   -3.333642  -3.804348  \n",
       "1225   -0.188501  -3.985507  \n",
       "1226    0.938974  -3.079710  \n",
       "1227    1.668250  -1.449275  \n",
       "\n",
       "[1228 rows x 92 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ta import add_all_ta_features\n",
    "from pykrx import stock\n",
    "\n",
    "print()\n",
    "print(f\"--- Loading stock data for ticker: {TARGET_TICKER} ({ticker_code}) ---\")\n",
    "df_stock = pd.read_parquet(f\"{ticker_code}.parquet\")\n",
    "\n",
    "# Load OHLCV data for the specified ticker and date range\n",
    "print(f\"--- Loading OHLCV data from KRX for ticker: {TARGET_TICKER} ({ticker_code}) ---\")\n",
    "df_ohlcv = stock.get_market_ohlcv_by_date(start_date, end_date, ticker_code)\n",
    "df_ohlcv.reset_index(inplace=True)\n",
    "df_ohlcv.rename(columns={'ë‚ ì§œ':'date', 'ì‹œê°€':'open', 'ê³ ê°€':'high', 'ì €ê°€':'low', 'ì¢…ê°€':'close', 'ê±°ë˜ëŸ‰':'volume'}, inplace=True)\n",
    "\n",
    "print(\"--- Adding technical indicators using 'ta' library ---\")\n",
    "# Add all technical indicators using the 'ta' library\n",
    "df_ohlcv = add_all_ta_features(\n",
    "    df_ohlcv, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"volume\", fillna=True\n",
    ")\n",
    "\n",
    "# Remove unnecessary columns and handle missing values\n",
    "df_ohlcv.drop(columns=['open', 'high', 'low', 'volume', 'close', 'ë“±ë½ë¥ '], inplace=True)\n",
    "\n",
    "# Merge the existing stock data with OHLCV data on 'date'\n",
    "df_stock = pd.merge(df_stock, df_ohlcv, on='date', how='left')\n",
    "\n",
    "print(f\"\\n--- Data shape after adding indicators: {df_stock.shape}\")\n",
    "\n",
    "df_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6ae1aefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PyTorch Version ---\n",
      "2.7.0+xpu\n",
      "CUDA Available: False\n",
      "XPU Available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# torch.cuda.is_available()\n",
    "# torch.xpu.is_available()\n",
    "print(\"--- PyTorch Version ---\")\n",
    "print(torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"XPU Available:\", torch.xpu.is_available())\n",
    "device = 'cuda' if torch.cuda.is_available() else 'xpu' if torch.xpu.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ad1e84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0baf0715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "\n",
    "# Split the df_stock into training, validation, and test sets (60% train, 20% validation, 20% test)\n",
    "train_df, test_df = train_test_split(\n",
    "    df_stock, test_size=0.2, shuffle=False\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.25, shuffle=False  # 0.8 * 0.25 = 0.2\n",
    ")\n",
    "\n",
    "# features_to_use = df_stock.select_dtypes(include=[np.number]).columns.drop(['target'])\n",
    "features_to_use = [\n",
    "    # --- Volume Indicators ---\n",
    "    \"volume_obv\", \"volume_cmf\",\n",
    "    # --- Volatility Indicators ---\n",
    "    \"volatility_bbh\", \"volatility_bbl\", \"volatility_kch\", \"volatility_kcl\",\n",
    "    # --- Trend Indicators ---\n",
    "    \"trend_macd_diff\", \"trend_sma_fast\", \"trend_sma_slow\", \"trend_ema_fast\", \"trend_ema_slow\",\n",
    "    \"trend_ichimoku_a\", \"trend_ichimoku_b\", \"trend_psar_up\", \"trend_psar_down\",\n",
    "    # --- Momentum Indicators ---\n",
    "    \"momentum_rsi\", \"momentum_stoch\", \"momentum_stoch_signal\",\n",
    "    # --- Other Key Features ---\n",
    "    \"close\", \"kospi_close\", \"sentiment\"\n",
    "]\n",
    "\n",
    "to_pct_change = [\n",
    "    \"close\", \"kospi_close\", \"volatility_dch\", \"trend_sma_slow\",\n",
    "    \"volume_obv\", \"volatility_bbh\", \"volatility_bbl\",\n",
    "    \"volatility_kch\", \"volatility_kcl\", \"trend_sma_fast\"\n",
    "]\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    for col in to_pct_change:\n",
    "        df[col] = df[col].pct_change().fillna(0)\n",
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[features_to_use])\n",
    "train_df[features_to_use] = scaler.transform(train_df[features_to_use])\n",
    "val_df[features_to_use] = scaler.transform(val_df[features_to_use])\n",
    "test_df[features_to_use] = scaler.transform(test_df[features_to_use])\n",
    "\n",
    "# Create sliding windows for the time series data\n",
    "def create_sliding_windows(data, sequence_length=10):\n",
    "    xs, ys = [], []\n",
    "    target_col = 'target_label'\n",
    "\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        x = data[features_to_use].iloc[i:(i + sequence_length)].values\n",
    "        y = data[target_col].iloc[i + sequence_length ]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Drop first row\n",
    "train_df = train_df.drop(index=train_df.index[0])\n",
    "val_df = val_df.drop(index=val_df.index[0])\n",
    "test_df = test_df.drop(index=test_df.index[0])\n",
    "\n",
    "SEQ_LENGTH = 10 # Length of the sliding window\n",
    "# Create sliding windows for the training, validation, and test sets\n",
    "X_train, y_train = create_sliding_windows(train_df, SEQ_LENGTH)\n",
    "X_val, y_val = create_sliding_windows(val_df, SEQ_LENGTH)\n",
    "X_test, y_test = create_sliding_windows(test_df, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b40fbd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data shape before augmentation: X_train: (725, 10, 21), y_train: (725,)\n",
      "\n",
      "--- Not using data augmentation ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data augmentation using noise\n",
    "augmentation = 1\n",
    "\n",
    "def augment_data_noise(X_input, y_input, n, noise_level=0.01):\n",
    "    if n <= 1:\n",
    "        return X_input, y_input\n",
    "    print(f\"\\n--- Data augmentation: {n} times (Noise level: {noise_level}) ---\")\n",
    "        \n",
    "    augmented_X = [X_input]\n",
    "    augmented_y = [y_input]\n",
    "    \n",
    "    for _ in range(n - 1):\n",
    "        noise = np.random.normal(loc=0.0, scale=noise_level, size=X_input.shape)\n",
    "        augmented_X.append(X_input + noise)\n",
    "        augmented_y.append(y_input)\n",
    "        \n",
    "    X_final = np.concatenate(augmented_X, axis=0)\n",
    "    y_final = np.concatenate(augmented_y, axis=0)\n",
    "    \n",
    "    return X_final, y_final\n",
    "\n",
    "print(f\"\\n--- Data shape before augmentation: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "if augmentation > 1:\n",
    "    X_train, y_train = augment_data_noise(X_train, y_train, augmentation)\n",
    "    \n",
    "    print(f\"--- Data shape after augmentation: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "else:\n",
    "    print(\"\\n--- Not using data augmentation ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3489d862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data shape before SMOTE: X_train: (725, 10, 21), y_train: (725,)\n",
      "--- Class distribution before SMOTE: [222 310 193]\n",
      "--- Data shape after SMOTE: X_train: (930, 10, 21), y_train: (930,)\n",
      "--- Class distribution after SMOTE: [310 310 310]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "print(f\"--- Data shape before SMOTE: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"--- Class distribution before SMOTE: {np.bincount(y_train)}\")\n",
    "\n",
    "# The shape of X_train is (samples, sequence_length, features).\n",
    "# SMOTE expects 2D data, so we need to reshape it.\n",
    "n_samples, seq_len, n_features = X_train.shape\n",
    "X_train_reshaped = X_train.reshape(n_samples, seq_len * n_features)\n",
    "\n",
    "# Initialize and apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_reshaped, y_train) # type: ignore\n",
    "\n",
    "# Reshape the data back to its original 3D format (samples, sequence, features)\n",
    "X_train = X_train_smote.reshape(-1, seq_len, n_features) # type: ignore\n",
    "y_train = y_train_smote\n",
    "\n",
    "print(f\"--- Data shape after SMOTE: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"--- Class distribution after SMOTE: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d0c653f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors and move to the appropriate device\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "y_val_tensor = torch.LongTensor(y_val).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "b21773fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Transformer Model Structure ---\n",
      "StockTransformer(\n",
      "  (encoder): Linear(in_features=21, out_features=128, bias=True)\n",
      "  (pos_encoder): PositionalEncoding()\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Positional Encoding for Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=8192):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(self.pe, torch.Tensor):\n",
    "            raise ValueError(\"Positional encoding buffer 'pe' is not a tensor. Ensure it is properly initialized.\")\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "# Stock Transformer Model\n",
    "class StockTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dropout=0.2):\n",
    "        super(StockTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.encoder = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, 3)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.decoder(output[:, -1, :])\n",
    "        return output\n",
    "    \n",
    "input_dim = X_train.shape[2] # Number of features in the input data\n",
    "\n",
    "d_model = 128   # Embedding dimension\n",
    "nhead = 4       # Heads in multi-head attention\n",
    "num_layers = 4  # Encoder layers\n",
    "\n",
    "model = StockTransformer(input_dim, d_model, nhead, num_layers).to(device)\n",
    "print(\"--- Transformer Model Structure ---\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "4e5e1dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LSTM Model Structure ---\n",
      "LSTMModel(\n",
      "  (lstm): LSTM(21, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Replace the StockTransformer class in main.ipynb with this LSTMModel\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_prob\n",
    "        )\n",
    "        # Define a fully connected layer to map the LSTM output to the desired output size\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM returns output and a tuple of the final hidden and cell states\n",
    "        # We only need the output of the last time step\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Get the output of the last time step\n",
    "        last_time_step_out = lstm_out[:, -1, :]\n",
    "        # Pass the last time step's output to the fully connected layer\n",
    "        out = self.fc(last_time_step_out)\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = X_train.shape[2]\n",
    "hidden_dim = 64  # Size of the LSTM hidden layer\n",
    "num_layers = 2   # Number of LSTM layers\n",
    "output_dim = 3   # Number of classes (í•˜ë½, ë³´í•©, ìƒìŠ¹)\n",
    "dropout = 0.2\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim, dropout).to(device)\n",
    "\n",
    "print(\"--- LSTM Model Structure ---\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9cef8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader creation\n",
    "batch_size = 8\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "eval_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2ec0e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Implementation\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta: float =0.0, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose and \\\n",
    "            (self.counter == 1 or self.patience // 2 == self.counter or self.counter % 10 == 0):\n",
    "                print(f'EarlyStopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} -> {val_loss:.6f}). Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "767c323f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting model training ğŸš€ ---\n",
      "Epoch [1/1000], Train Loss: 1.103388, Val Loss: 1.107350\n",
      "Validation loss decreased (inf -> 1.107350). Saving model ...\n",
      "Epoch [2/1000], Train Loss: 1.096905, Val Loss: 1.107328\n",
      "Validation loss decreased (1.107350 -> 1.107328). Saving model ...\n",
      "Epoch [3/1000], Train Loss: 1.090472, Val Loss: 1.108500\n",
      "EarlyStopping counter: 1/20\n",
      "Epoch [4/1000], Train Loss: 1.083952, Val Loss: 1.110815\n",
      "Epoch [5/1000], Train Loss: 1.077979, Val Loss: 1.114077\n",
      "Epoch [6/1000], Train Loss: 1.072607, Val Loss: 1.117237\n",
      "Epoch [7/1000], Train Loss: 1.068663, Val Loss: 1.119651\n",
      "Epoch [8/1000], Train Loss: 1.059419, Val Loss: 1.119933\n",
      "Epoch [9/1000], Train Loss: 1.058639, Val Loss: 1.120220\n",
      "Epoch [10/1000], Train Loss: 1.048625, Val Loss: 1.120526\n",
      "Epoch [11/1000], Train Loss: 1.058234, Val Loss: 1.120765\n",
      "Epoch [12/1000], Train Loss: 1.048974, Val Loss: 1.121019\n",
      "EarlyStopping counter: 10/20\n",
      "Epoch [13/1000], Train Loss: 1.057516, Val Loss: 1.121246\n",
      "Epoch [14/1000], Train Loss: 1.056306, Val Loss: 1.121271\n",
      "Epoch [15/1000], Train Loss: 1.055622, Val Loss: 1.121298\n",
      "Epoch [16/1000], Train Loss: 1.056514, Val Loss: 1.121322\n",
      "Epoch [17/1000], Train Loss: 1.056251, Val Loss: 1.121346\n",
      "Epoch [18/1000], Train Loss: 1.056397, Val Loss: 1.121371\n",
      "Epoch [19/1000], Train Loss: 1.055958, Val Loss: 1.121393\n",
      "Epoch [20/1000], Train Loss: 1.056155, Val Loss: 1.121396\n",
      "Epoch [21/1000], Train Loss: 1.055779, Val Loss: 1.121398\n",
      "Epoch [22/1000], Train Loss: 1.055207, Val Loss: 1.121401\n",
      "EarlyStopping counter: 20/20\n",
      "Early stopping triggered. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "# Loss function with class weights and optimizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class_counts = np.bincount(y_train) \n",
    "total_samples = len(y_train)\n",
    "class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "eval_class_counts = np.bincount(y_val)\n",
    "eval_total_samples = len(y_val)\n",
    "eval_class_weights = eval_total_samples / (len(eval_class_counts) * eval_class_counts)\n",
    "eval_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "eval_criterion = nn.CrossEntropyLoss(weight=eval_weights_tensor)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "# EarlyStopping setting: if val_loss does not improve for 'patience' epochs, stop training\n",
    "early_stopping = EarlyStopping(patience=20, verbose=True, path=f'{ticker_code}_best_model.pt')\n",
    "\n",
    "print(\"\\n--- Starting model training ğŸš€ ---\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Model training loop\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Model evaluation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in eval_loader:\n",
    "            output = model(batch_X)\n",
    "            loss = eval_criterion(output, batch_y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(eval_loader)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}')\n",
    "    \n",
    "    # Early stopping check\n",
    "    early_stopping(avg_val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268b64f",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "12d0c5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final evaluation with the best model ---\n",
      "--- Train Acc ---\n",
      "Accuracy: 0.3892\n",
      "--- Eval Acc ---\n",
      "Accuracy: 0.2298\n",
      "--- Test Acc ---\n",
      "Accuracy: 0.3617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          í•˜ë½       0.38      0.60      0.46        90\n",
      "          ë³´í•©       0.34      0.41      0.37        76\n",
      "          ìƒìŠ¹       0.00      0.00      0.00        69\n",
      "\n",
      "    accuracy                           0.36       235\n",
      "   macro avg       0.24      0.34      0.28       235\n",
      "weighted avg       0.25      0.36      0.30       235\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devwo\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\devwo\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\devwo\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "model.load_state_dict(torch.load(f'{ticker_code}_best_model.pt'))\n",
    "\n",
    "def eval(loader, calssification_report=False):\n",
    "    # Loader for evaluation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            output = model(batch_X)\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy and classification report\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    if calssification_report:\n",
    "        print(classification_report(all_labels, all_preds, target_names=['í•˜ë½', 'ë³´í•©', 'ìƒìŠ¹']))\n",
    "\n",
    "print(\"\\n--- Final evaluation with the best model ---\")\n",
    "print(\"--- Train Acc ---\")\n",
    "eval(train_loader)\n",
    "print(\"--- Eval Acc ---\")\n",
    "eval(eval_loader)\n",
    "print(\"--- Test Acc ---\")\n",
    "eval(test_loader, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe62e9e",
   "metadata": {},
   "source": [
    "### After Resoning with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51091fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc5d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_reason(date, company_name, prediction, news_titles):\n",
    "    prediction_text = \"ìƒìŠ¹\" if prediction == 1 else \"í•˜ë½\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    ## AI ì£¼ê°€ ì˜ˆì¸¡ ë¶„ì„ ë¦¬í¬íŠ¸\n",
    "\n",
    "    - **ë¶„ì„ ëŒ€ìƒ:** {company_name}\n",
    "    - **ë¶„ì„ ê¸°ì¤€ì¼:** {date}\n",
    "    - **ì˜ˆì¸¡ ê²°ê³¼:** **{prediction_text}** ì˜ˆìƒ\n",
    "\n",
    "    **## ë¶„ì„ ê·¼ê±°:**\n",
    "    ì•„ë˜ëŠ” ë¶„ì„ ê¸°ì¤€ì¼ì— ìˆ˜ì§‘ëœ ì£¼ìš” ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "    ---\n",
    "    {news_titles}\n",
    "    ---\n",
    "    \n",
    "    **## ì§€ì‹œì‚¬í•­:**\n",
    "    ë‹¹ì‹ ì€ ìµœê³ ì˜ ê¸ˆìœµ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ìœ„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬, \"{company_name}ì˜ ì£¼ê°€ê°€ ì™œ {prediction_text}í•  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë˜ëŠ”ì§€\"ì— ëŒ€í•œ **ë…¼ë¦¬ì ì´ê³  ìƒì„¸í•œ ì´ìœ **ë¥¼ ì „ë¬¸ê°€ì˜ ì‹œê°ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ê¸ì •ì  ìš”ì¸ê³¼ ë¶€ì •ì  ìš”ì¸ì„ ë‚˜ëˆ„ì–´ ì„¤ëª…í•˜ê³ , ìµœì¢… ê²°ë¡ ì„ ì œì‹œí•´ì£¼ì„¸ìš”.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt\n",
    "    )\n",
    "\n",
    "    return response.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
